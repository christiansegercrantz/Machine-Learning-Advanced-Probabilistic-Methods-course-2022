{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2d7f17c6c790de73b8150205ef2abef",
     "grade": false,
     "grade_id": "cell-81c5a400584e4a8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CS-E4820 Machine Learning: Advanced Probabilistic Methods (Spring 2022)\n",
    "\n",
    "Pekka Marttinen, Prayag Tiwari, Vishnu Raj, Tianyu Cui, Yogesh Kumar, Antti Pöllänen, Louis Filstroff, Alex Aushev, Zheyang Shen, Nikitin Alexander , Sebastiaan De Peuter.\n",
    "\n",
    "## Exercise 3, due on Tuesday February 8 at 23:50.\n",
    "\n",
    "### Contents\n",
    "1. Problem 1: Poisson-Gamma\n",
    "2. Problem 2: Multivariate Gaussian\n",
    "3. Problem 3: Posterior of regression weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38bb2e5ebde49e1760a076b099d6e5a6",
     "grade": false,
     "grade_id": "cell-573bbaa2ef327be0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Problem 1: Poisson-Gamma\n",
    "\n",
    "Suppose you have $N$ i.i.d. observations $\\mathbf{x}= \\{x_i\\}_{i=1}^N$ from a $\\operatorname{Poisson}(\\lambda)$ distribution with a rate parameter $\\lambda$ that has a conjugate prior \n",
    "\n",
    "$$\\lambda \\sim \\operatorname{Gamma}(a,b)$$\n",
    "\n",
    "with the shape and rate hyperparameters $a$ and $b$. Derive the posterior distribution $\\lambda|\\bf{x}$.\n",
    "\n",
    "Write your solutions in LateX or attach a picture in the answer cell provided below. You can add a picture using the command ```!(imagename_in_the_folder.jpg)```. Latex in here works similarly as you would write it normally! You can use some of the definitions from the exercise description as a reference. The list of valid Latex commands in Jypyter notebook can be found here: http://www.onemathematicalcat.org/MathJaxDocumentation/TeXSyntax.htm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\tp(\\lambda \\mid X) &= \\frac{p(X \\mid \\lambda)p(\\lambda)}{p(X)} \\propto p(X \\mid \\lambda) p(\\lambda)\\\\\n",
    "\tp(\\lambda) &= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}e^{-\\beta \\lambda}\\\\\n",
    "\tp(X \\mid \\lambda) &=\\prod_{i=1}^{n} \\frac{\\lambda^{x_i}e^{-\\lambda}}{x_i!} \\\\\n",
    "\t\\implies p(\\lambda \\mid X) &= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}e^{-\\beta \\lambda} \\prod_{i=1}^{n} \\frac{\\lambda^{x_i}e^{-\\lambda}}{x_i!}\\\\\n",
    "\t&= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}e^{-\\beta \\lambda}  \\frac{\\lambda^{\\sum_{i=1}^{n}x_i}e^{-n\\lambda}}{\\prod_{i=1}^{n}(x_i)!}\\\\\n",
    "\t&\\propto \\lambda^{\\alpha-1}e^{-\\beta \\lambda}  \\lambda^{\\sum_{i=1}^{n}x_i}e^{-n\\lambda}\\\\\n",
    "\t&= \\lambda^{\\sum_{i=1}^{n}(x_i)+\\alpha-1}e^{-\\lambda(\\beta+n)}\\\\\n",
    "\t\\implies p(\\lambda \\mid X) &\\propto Gam(\\sum_{i=1}^{n}(x_i)+\\alpha,\\beta+n)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2d1bd8470ba33c5aa2596654e3cefbc",
     "grade": false,
     "grade_id": "cell-7fdfccb96ae5c3d1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Problem 2: Multivariate Gaussian\n",
    "\n",
    "Suppose we have $N$ i.i.d. observations $\\mathbf{X} = \\{\\mathbf{x}_i\\}_{i=1}^N$ from a multivariate Gaussian distribution $$\\mathbf{x}_i \\mid \\boldsymbol{\\mu} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$$ with unknown mean parameter $\\boldsymbol{\\mu}$  and a known covariance matrix $\\boldsymbol{\\Sigma}$. As prior information on the mean parameter we have $$ \\boldsymbol{\\mu} \\sim \\mathcal{N}(\\mathbf{m_0}, \\mathbf{S_0}). $$\n",
    "\n",
    "__(a)__ Derive the posterior distribution $p(\\boldsymbol{\\mu}|\\mathbf{X})$ of the mean parameter $\\boldsymbol{\\mu}$. Write your solution in LateX or attach a picture of the solution in the cell below.\n",
    "\n",
    "__(b)__ Compare the Bayesian estimate (posterior mean) to the maximum likelihood estimate by generating $N=10$ observations from the bivariate Gaussian \n",
    "        $$\\mathcal{N}\\left(\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}, \\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix}\\right).$$\n",
    "For this you can use the Python function [numpy.random.normal](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html), making use of the fact that the elements of the bivariate random vectors are independent. In the Bayesian case, use the prior with $\\mathbf{m_0} = [0,0]^T$ and $\\mathbf{S_0} = [\\begin{smallmatrix}0.1 & 0 \\\\ 0 & 0.1\\end{smallmatrix}]$. Report both estimates. Is the Bayesian estimate closer to the true value $\\boldsymbol{\\mu} = [0,0]^T$? Use the code template given below (after the answer cell) to complete your answer.\n",
    "\n",
    "Write your solutions to __(a)__ and __(b)__ in LateX or attach a picture in the answer cell provided below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "p(\\mu \\mid X) &= \\frac{p(X \\mid \\mu, \\Sigma)p(\\mu)}{p(X)} \\propto p(X \\mid \\mu, \\Sigma) p(\\mu) \\\\\n",
    "\\mathcal{N}(m_0,S_0) &= \\frac{1}{\\sqrt{(2\\pi)^D \\det(S_0)}}\\exp\\left(-\\frac{1}{2}(\\mu-m_0)^\\top S_0^{-1}(\\mu-m_0)\\right)\\\\\n",
    "\\mathcal{N}(\\mu,\\Sigma) &=\\prod_{i=1}^n \\frac{1}{\\sqrt{(2\\pi)^D \\det(\\Sigma)}}\\exp\\left(-\\frac{1}{2}(x_i-\\mu)^\\top \\Sigma^{-1}(x_i-\\mu)\\right)\\\\\n",
    "\\implies p(\\mu \\mid X) &\\propto \\frac{1}{\\sqrt{(2\\pi)^D \\det(S_0)}}\\exp\\left(-\\frac{1}{2}(\\mu-m_0)^\\top S_0^{-1}(\\mu-m_0)\\right) \\prod_{i=1}^n \\frac{1}{\\sqrt{(2\\pi)^D \\det(\\Sigma)}}\\exp\\left(\\frac{1}{2}(x_i-\\mu)^\\top \\Sigma^{-1}(x_i-\\mu)\\right)\\\\\n",
    "& =\\frac{1}{\\sqrt{(2\\pi)^D \\det(S_0)}}\\frac{1}{\\sqrt{(2\\pi)^D \\det(\\Sigma)}} \\prod_{i=1}^n \\exp\\left(\\frac{1}{2}(\\mu-m_0)^\\top S_0^{-1}(\\mu-m_0)\\right) \\exp\\left(-\\frac{1}{2}(x_i-\\mu)^\\top \\Sigma^{-1}(x_i-\\mu)\\right)\\\\\n",
    "& \\propto \\prod_{i=1}^n \\exp\\left(-\\frac{1}{2}(\\mu-m_0)^\\top S_0^{-1}(\\mu-m_0) - \\frac{1}{2}(x_i-\\mu)^\\top \\Sigma^{-1}(x_i-\\mu)\\right)\\\\\n",
    "& =  \\exp\\left(-\\frac{1}{2}(\\mu-m_0)^\\top S_0^{-1}(\\mu-m_0)- \\frac{1}{2}\\sum_{i=1}^n\\left((x_i-\\mu)^\\top \\Sigma^{-1}(x_i-\\mu)\\right)\\right)\n",
    "\\end{align}$$\n",
    "\n",
    "We complete the square for both parts as \n",
    "\n",
    "$\\frac{1}{2}(\\mu-m_0)^\\top S_0^{-1}(\\mu-m_0) = \\frac{1}{2}\\left(\\mu^\\top S_0^{-1}\\mu - m_0^\\top S_0^{-1}\\mu+ m_0^\\top S_0^{-1} m_0 \\right)$\n",
    "\n",
    "$\\frac{1}{2}(x_i-\\mu)^\\top \\Sigma^{-1}(x_i-\\mu) = \\frac{1}{2}\\left(x_i^\\top \\Sigma^{-1} x_i-\\mu^\\top \\Sigma^{-1} x_i+ \\mu^\\top \\Sigma^{-1}\\mu \\right)$\n",
    "\n",
    "Since the covariance matrices are symmetric matrices we used do the following $(\\Sigma^{-1})^\\top = (\\Sigma^{\\top})^{-1} = (\\Sigma^{-1})$\n",
    "\n",
    "$$\\begin{align} &=  \\exp\\left(-\\frac{1}{2}\\left(\\mu^\\top S_0^{-1}\\mu-m_0^\\top S_0^{-1}\\mu + m_0^\\top S_0^{-1} m_0\\right)- \\frac{1}{2}\\sum_{i=1}^n\\left(x_i^\\top \\Sigma^{-1} x_i-\\mu^\\top \\Sigma^{-1} x_i + \\mu^\\top \\Sigma^{-1}\\mu\\right)\\right) \\\\\n",
    "&=  \\exp\\left(-\\frac{1}{2}\\left(\\mu^\\top S_0^{-1}\\mu-m_0^\\top S_0^{-1}\\mu + m_0^\\top S_0^{-1} m_0\\right) - \\frac{1}{2}n\\mu^\\top \\Sigma^{-1}\\mu - \\frac{1}{2}\\sum_{i=1}^n\\left(x_i^\\top \\Sigma^{-1} x_i-\\mu^\\top \\Sigma^{-1} x_i\\right)\\right), \\qquad | m_0^\\top S_0^{-1}\\mu= (m_0^\\top S_0^{-1}\\mu)^\\top = \\mu^\\top S_0^{-1}m_0 \\text{ since scalar} \\\\\n",
    "&=  \\exp\\left(-\\frac{1}{2}\\left(\\mu^\\top S_0^{-1}\\mu + m_0^\\top S_0^{-1} m_0 -\\mu^\\top S_0^{-1}m_0 + n\\mu^\\top \\Sigma^{-1}\\mu+ \\sum_{i=1}^n\\left(x_i^\\top \\Sigma^{-1} x_i-\\mu^\\top \\Sigma^{-1} x_i\\right)\\right)\\right)\\\\\n",
    "&=  \\exp\\left(-\\frac{1}{2}\\left(\\mu^\\top\\left(S_0^{-1}\\mu - n \\Sigma^{-1}\\mu\\right) -\\mu^\\top S_0^{-1}m_0 + m_0^\\top S_0^{-1} m_0+ \\sum_{i=1}^nx_i^\\top \\Sigma^{-1} x_i-\\mu^\\top \\Sigma^{-1}\\sum_{i=1}^n x_i\\right)\\right)\\\\\n",
    "&= \\exp\\left(-\\frac{1}{2}\\left(\\mu^\\top\\left(S_0^{-1} + n \\Sigma^{-1}\\right)\\mu - \\mu^\\top\\left( S_0^{-1}m_0 - \\Sigma^{-1}\\sum_{i=1}^n x_i\\right) + m_0^\\top S_0^{-1} m_0+ \\sum_{i=1}^nx_i^\\top \\Sigma^{-1} x_i\\right)\\right) \\\\\n",
    "&\\propto \\exp\\left(-\\frac{1}{2}\\left(\\mu^\\top\\left(S_0^{-1} + n \\Sigma^{-1}\\right)\\mu - \\mu^\\top\\left( S_0^{-1}m_0 - \\Sigma^{-1}\\sum_{i=1}^n x_i\\right)\\right)\\right)\\\\\n",
    "&= \\exp\\left(\\frac{1}{2}\\left(-\\mu^\\top\\left(S_0^{-1} + n \\Sigma^{-1}\\right)\\mu + \\left( S_0^{-1}m_0 - \\Sigma^{-1}\\sum_{i=1}^n x_i\\right)^\\top \\mu \\right)\\right)\n",
    "\\end{align}$$\n",
    "\n",
    "From here we can see, from completing the square, the gaussian distribution with the the following parameters\n",
    "\n",
    "$\\Sigma_p = \\left(S_0^{-1} + n \\Sigma^{-1}\\right)^{-1}$\n",
    "and \n",
    "$\\mu_p = \\left(S_0^{-1} + n \\Sigma^{-1}\\right)^{-1}\\left(S_0^{-1}m_0 - \\Sigma^{-1}\\sum_{i=1}^n x_i\\right) \n",
    "= \\Sigma_p \\left(S_0^{-1}m_0 - \\Sigma^{-1}\\sum_{i=1}^n x_i\\right)$\n",
    "and get\n",
    "$ \\mathcal{N}\\left(\\mu \\mid \\mu_p, \\Sigma_p\\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af88913931d4649db8324917756a5b72",
     "grade": false,
     "grade_id": "cell-e6a09ef8bf1f72d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE: [-0.3487216562662467, -0.22338008226845346]\n",
      "Posterior mean: [0.17436083 0.11169004]\n"
     ]
    }
   ],
   "source": [
    "# template for 2(b)\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "S0 = np.array([[0.1, 0],[0, 0.1]])\n",
    "Sigma = np.array([[1, 0],[0, 1]])\n",
    "N = 10\n",
    "\n",
    "m0 = np.array([0, 0])\n",
    "\n",
    "# Sample N bivariate normal vectors\n",
    "# compute MLE and also the posterior mean solution\n",
    "x = np.random.normal(size = (10,2))\n",
    "\n",
    "Sigma_p = inv(inv(S0)+N*inv(Sigma))\n",
    "mu_p = Sigma_p@(inv(S0)@m0-inv(Sigma)@np.sum(x,axis=0))\n",
    "\n",
    "mle = [np.mean(x[:,0]), np.mean(x[:,1])] \n",
    "posterior_mean =  mu_p\n",
    "\n",
    "print(\"MLE:\", mle)\n",
    "print(\"Posterior mean:\", posterior_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the run, but in general is the Beysian estimate closer to the true value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddf1e85bf2fabec6a07c3676a5945499",
     "grade": false,
     "grade_id": "cell-6f265c79745ea700",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 3: Posterior of regression weights\n",
    "\n",
    "Suppose $y_{i}=\\mathbf{w}^{T}\\mathbf{x}_{i}+\\epsilon_{i},$ for $i=1,\\ldots,n,$ where $\\epsilon_{i}\\sim \\mathcal{N}(0,\\beta^{-1})$. Assume a prior $$\\mathbf{w} \\sim \\mathcal{N} (\\mathbf{0},\\alpha^{-1}\\mathbf{I}).$$ Use 'completing the square' to show that the posterior of $\\mathbf{w}$ is given by $p(\\mathbf{w} \\mid \\mathbf{y}, \\mathbf{x}, \\alpha, \\beta)=\\mathcal{N}(\\mathbf{w} \\mid \\mathbf{m}, \\mathbf{S}),$ where \n",
    "\\begin{align*}\n",
    "    \\mathbf{S} &= \\left( \\alpha \\mathbf{I} + \\beta \\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^T \\right)^{-1}\\;, \\\\\n",
    "    \\mathbf{m} &= \\beta \\mathbf{S} \\sum_{i=1}^{n} y_i \\mathbf{x}_i.\n",
    "\\end{align*}\n",
    "\n",
    "Write your solution in LateX or attach a picture of the solution in the cell below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\tp(\\mathbf{w} \\mid y, X, \\alpha, \\beta) &= \\frac{p(y \\mid \\mathbf{w}, X, \\alpha, \\beta)p(\\mathbf{w} \\mid X, \\alpha, \\beta)}{p(y\\mid X, \\alpha, \\beta)} \\\\\n",
    "\t&\\propto p(y \\mid \\mathbf{w, X, \\alpha, \\beta}) p(\\mathbf{w}\\mid X, \\alpha, \\beta)\\\\\t\n",
    "\tp(\\mathbf{w}\\mid X, \\alpha, \\beta) &= \\mathcal{N} (\\mathbf{0},\\alpha^{-1}\\mathbf{I})\\\\\n",
    "\t&= \\frac{1}{\\sqrt{(2\\pi)^D \\det((\\alpha^{-1} \\mathbf{I}))}}\\exp\\left(-\\frac{1}{2}\\mathbf{w}^\\top (\\alpha^{-1} \\mathbf{I})^{-1}\\mathbf{w}\\right)\\\\\n",
    "\tp(y \\mid \\mathbf{w}, X, \\alpha, \\beta ) &=\\prod_{i=1}^{N} p(y_i \\mid \\mathbf{w}, x_i, \\alpha, \\beta) \\\\\n",
    "\t&= \\prod_{i=1}^{N} \\mathcal{N}( \\mathbf{w}^\\top x_i, \\beta^{-1}) \\\\\n",
    "\t&= \\prod_{i=1}^{N}\\frac{1}{\\sqrt{(2\\pi)^D \\det((\\beta^{-1}))}}\\exp\\left(-\\frac{1}{2}(y_i - \\mathbf{w}^\\top x_i)^\\top  \\beta (y_i - \\mathbf{w}^\\top x_i)\\right)\\\\\n",
    "\t\\implies p(\\mathbf{w} \\mid y, X, \\alpha, \\beta) &= \\mathcal{N} (\\mathbf{0},\\alpha^{-1}\\mathbf{I}) \\prod_{i=1}^{N} \\mathcal{N}( \\mathbf{w}^\\top x_i, \\beta^{-1})\\\\\n",
    "\t&\\propto \\exp\\left(-\\frac{1}{2}\\mathbf{w}^\\top (\\alpha^{-1} \\mathbf{I})^{-1}\\mathbf{w}\\right)\\prod_{i=1}^{N}\\exp\\left(-\\frac{1}{2}(y_i - \\mathbf{w}^\\top x_i)^\\top  \\beta (y_i - \\mathbf{w}^\\top x_i)\\right)\\\\\n",
    "\t& = \\exp\\left(-\\frac{1}{2}\\mathbf{w}^\\top (\\alpha^{-1} \\mathbf{I})^{-1}\\mathbf{w}-\\frac{1}{2}\\sum_{i=1}^{N}\\left((y_i - \\mathbf{w}^\\top x_i)^\\top  \\beta (y_i - \\mathbf{w}^\\top x_i)\\right)\\right)\\\\\n",
    "\t&=\\exp\\left(-\\frac{1}{2}\\alpha\\mathbf{w}^\\top \\mathbf{w}-\\frac{1}{2}\\sum_{i=1}^{N}\\left((y_i - \\mathbf{w}^\\top x_i)^\\top  \\beta (y_i - \\mathbf{w}^\\top x_i)\\right)\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "By completing the square we get\n",
    "$$\n",
    "\\begin{align}\n",
    "\t(y_i - \\mathbf{w}^\\top x_i)^\\top  \\beta (y_i - \\mathbf{w}^\\top x_i) &= \n",
    "\ty_i^\\top \\beta y_i - 2(\\beta \\mathbf{w}^\\top x_i)^\\top y_i + (\\beta^\\top \\mathbf{w} x_i)^\\top \\mathbf{w}^\\top x_i\\\\\n",
    "\t&= y_i^\\top \\beta y_i - 2x_i^\\top\\mathbf{w}\\beta y_i + x_i^\\top\\mathbf{w}\\beta \\mathbf{w}^\\top x_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\t&\\implies \\exp\\left(-\\frac{1}{2}\\alpha\\mathbf{w}^\\top \\mathbf{w}-\\frac{1}{2}\\sum_{i=1}^{N}\\left(y_i^\\top \\beta y_i - 2x_i^\\top\\mathbf{w}\\beta y_i + x_i^\\top\\mathbf{w}\\beta \\mathbf{w}^\\top x_i\\right)\\right)\\\\\n",
    "\t&= \\exp\\left(-\\frac{1}{2}\\alpha\\mathbf{w}^\\top \\mathbf{w}-\\frac{1}{2}\\sum_{i=1}^{N}y_i^\\top \\beta y_i +\\sum_{i=1}^{N} x_i^\\top\\mathbf{w}\\beta y_i -\\frac{1}{2}\\sum_{i=1}^{N} x_i^\\top\\mathbf{w}\\beta \\mathbf{w}^\\top x_i\\right) \\\\\n",
    "\t& \\propto \\exp\\left(-\\frac{1}{2}\\alpha\\mathbf{w}^\\top \\mathbf{w}\t +\\sum_{i=1}^{N} x_i^\\top\\mathbf{w}\\beta y_i -\\frac{1}{2}\\sum_{i=1}^{N} x_i^\\top\\mathbf{w}\\beta \\mathbf{w}^\\top x_i\\right) \\\\\n",
    "\t&= \\exp\\left(-\\frac{1}{2}\\mathbf{w}^\\top(\\alpha\\mathbf{I}) \\mathbf{w}\t  -\\frac{1}{2}\\beta\\sum_{i=1}^{N} \\mathbf{w}^\\top x_i x_i^\\top \\mathbf{w} +\\beta\\sum_{i=1}^{N} x_i^\\top y_i \\mathbf{w}\\right)\\\\\n",
    "\t&= \\exp\\left(-\\frac{1}{2}\\mathbf{w}^\\top\\underbrace{(\\alpha\\mathbf{I} + \\beta\\sum_{i=1}^{N} x_i x_i^\\top)}_{S^{-1}} \\mathbf{w} +\\underbrace{\\left(\\beta\\sum_{i=1}^{N}  y_i x_i\\right)^\\top}_{S^{-1}m^\\top} \\mathbf{w}\\right)\\\\\t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "By complething the square back again we could get the gaussian distribution with parameters\n",
    "\n",
    "$S = \\left(\\alpha\\mathbf{I} + \\beta\\sum_{i=1}^{N} x_i x_i^\\top\\right)^{-1}$\n",
    "and\n",
    "$\n",
    "m= S\\left(\\beta\\sum_{i=1}^{N}  y_i x_i\\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
