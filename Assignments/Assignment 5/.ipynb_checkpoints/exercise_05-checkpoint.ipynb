{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e92fb3725fc23e8bc2b845a67a835b3",
     "grade": false,
     "grade_id": "cell-5b335005bb36ae92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CS-E4820 Machine Learning: Advanced Probabilistic Methods (spring 2022)\n",
    "\n",
    "Pekka Marttinen, Prayag Tiwari, Vishnu Raj, Tianyu Cui, Yogesh Kumar, Antti Pöllänen, Louis Filstroff, Alex Aushev, Zheyang Shen, Nikitin Alexander , Sebastiaan De Peuter.\n",
    "\n",
    "## Exercise 5, due on Tuesday March 8 at 23:50.\n",
    "\n",
    "### Contents\n",
    "1. Problem 1: EM for missing observations\n",
    "2. Problem 2: Extension of 'simple example' from the lecture\n",
    "3. Problem 3: PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18c12b98afa6a333b6b4717029202b7d",
     "grade": false,
     "grade_id": "cell-298bb2ed1de6d806",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 1: EM for missing observations\n",
    "Suppose random variables $X_{i}$ follow a bivariate normal distribution $X_{i}\\sim \\mathcal{N}_{2}(0,\\Sigma)$, where\n",
    "$ \\Sigma = \\begin{bmatrix} 1 & \\rho\\\\ \\rho & 1 \\end{bmatrix} $.\n",
    "\n",
    "Suppose further that we have observations on $X_{1}=(X_{11},X_{12})^{T}$, $X_{2}=(X_{21},X_{22})^{T}$ and $X_{3}=(X_{31},X_{32})^{T}$, such that\n",
    "$X_{1}$ and $X_{3}$ are fully observed, and from $X_{2}$ we have observed only\n",
    "the second coordinate. Thus, our data matrix can be written as\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12}\\\\\n",
    "? & x_{22}\\\\\n",
    "x_{31} & x_{32}\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "where the rows correspond to the transposed observations $\\mathbf{x}_{1}^{T},\\mathbf{x}_{2}^{T},\\mathbf{x}_{3}^{T}$. Suppose we want to learn the unknown parameter $\\rho$ using the EM-algorithm. Denote the missing observation by $Z$ and derive the E-step of the algorithm, i.e., __(a)__ write the complete data log-likelihood $\\ell(\\rho)$, __(b)__ compute the posterior distribution of the missing observation, given the observed variables and current estimate for $\\rho$, and __(c)__ evaluate the expectation of $\\ell(\\rho)$ with respect to the posterior distribution of the missing observations.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "1. In general, for $X \\sim \\mathcal{N}_2(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, where $X=(X_1, X_2)^{T}$, $\\boldsymbol{\\mu}=(\\mu_1, \\mu_2)^{T}$ and $\\boldsymbol{\\Sigma} = \\begin{pmatrix} \n",
    "            \\sigma_1^{2} & \\rho\\sigma_{1}\\sigma_{2} \\\\ \n",
    "            \\rho\\sigma_{1}\\sigma_{2} & \\sigma_2^{2} \n",
    "            \\end{pmatrix}$, \n",
    "we have \n",
    "$$ X_1 \\mid X_2 = x_2 \\sim \\mathcal{N}\\left(\\mu_1 + \\frac{\\sigma_1}{\\sigma_2}\\rho(x_2-\\mu_2), (1-\\rho^2)\\sigma_1^{2}\\right),$$  with $\\rho$ being the correlation coefficient.\n",
    "2. For evaluating the expectation of $\\ell(\\rho)$, you can make use of the following two rules: \n",
    "    - $\\mathbf{x_2}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x_2} = trace(\\boldsymbol{\\Sigma}^{-1}\\mathbf{x_2x_2^T}).$\n",
    "    - if $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ then $\\langle{X^2}\\rangle = \\mu^2 + \\sigma^2$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\\begin{align}\n",
    "\t\\ell(\\rho) &= \\sum_{i=1}^{3}\\log p( \\mathbf{X_i} \\mid \\mathbf{\\rho})\\\\\n",
    "\tp( \\mathbf{X_i} \\mid \\mathbf{\\rho}) &= \\sum_{i=1}^{3}\\mathcal{N}(\\mathbf{X_i} \\mid 0, \\mathbf{\\Sigma(\\rho)})\\\\\n",
    "\t\\implies \\ell(\\rho) &= \\sum_{i=1}^{3}\\log \\left( \\mathcal{N}(\\mathbf{X_i} \\mid 0, \\mathbf{\\Sigma(\\rho)}) \\right) \\\\\n",
    "\t& = \\sum_{i=1}^{3}-\\frac{1}{2}\\log(\\det\\left( 2\\pi \\mathbf{\\Sigma(\\rho)}\\right)) + \\sum_{i=1}^{3}\\frac{1}{2}\\mathbf{X_i}^\\top \\mathbf{\\Sigma(\\rho)}^{-1}\\mathbf{X_i} \\\\\n",
    "\t& = -\\frac{3}{2}\\log(\\det\\left( 2\\pi \\mathbf{\\Sigma(\\rho)}\\right)) + \\sum_{i=1}^{3}\\frac{1}{2}\\mathbf{X_i}^\\top \\mathbf{\\Sigma(\\rho)}^{-1}\\mathbf{X_i}, \\qquad | \\mathbf{X_2} = \\begin{bmatrix} Z \\\\ x_{22}\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "### b)\n",
    "\n",
    "From the hint we can extrapolate that the posterior destribution of the missing value given the observed value and current estimate $\\rho_0$:\n",
    "\n",
    "\\begin{equation}\n",
    "    Z \\mid X_{22} = x_{22} \\sim \\mathcal{N} (\\rho_0 x_{22}, 1-\\rho_0^2)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### c) \n",
    "\n",
    "We want to compute the expectation with respect to the missing values $\\mathbf{Z}$. We see that the only term that depends $\\mathbf{Z}$ is $\\mathbf{X_2}$. We thus only need to count the following expectation:\n",
    "\n",
    "\\begin{align}\n",
    "\t\\mathbb{E}\\left( \\ell(\\rho)\\right) &= \\mathbb{E}\\left( -\\frac{3}{2}\\log(\\det\\left( 2\\pi \\mathbf{\\Sigma(\\rho)}\\right)) + \\sum_{i=1}^{3}\\frac{1}{2}\\mathbf{X_i}^\\top \\mathbf{\\Sigma(\\rho)}^{-1}\\mathbf{X_i}\\right) \\\\ \n",
    "\t&=  -\\frac{3}{2}\\log(\\det\\left( 2\\pi \\mathbf{\\Sigma(\\rho)}\\right)) + \\sum_{i=1\\lbrace 1,3 \\rbrace}\\frac{1}{2}\\mathbf{X_i}^\\top \\mathbf{\\Sigma(\\rho)}^{-1}\\mathbf{X_i} + \\frac{1}{2}\\mathbb{E}\\left(\\mathbf{X_2}^\\top \\mathbf{\\Sigma(\\rho)}^{-1}\\mathbf{X_2} \\right) \\\\\n",
    "\t\\mathbb{E}\\left(\\mathbf{X_2}^\\top \\mathbf{\\Sigma(\\rho)}^{-1}\\mathbf{X_2} \\right) \n",
    "\t&= \\mathbb{E}\\left(\\text{trace}(\\mathbf{\\Sigma(\\rho)}^{-1}\\mathbf{x_2x_2^T})\\right) \\\\\n",
    "\t&= \\text{trace}(\\mathbf{\\Sigma(\\rho)}^{-1}\\mathbb{E}(\\mathbf{x_2x_2^T})) \\\\\n",
    "\t&= \\text{trace}(\\mathbf{\\Sigma(\\rho)}^{-1}\\mathbb{E}\\left( \n",
    "\t\\begin{bmatrix}\n",
    "\t\t\\mathbf{Z}^2 & \\mathbf{Z} \\mathbf{x}_{22} \\\\\n",
    "\t\t\\mathbf{Z}x_{22} & \\mathbf{x}_{22}^2 \n",
    "\t\\end{bmatrix})\\right)  \\\\\n",
    "\t\\mathbb{E}\\left( \n",
    "\t\\begin{bmatrix}\n",
    "\t\t\\mathbf{Z}^2 & \\mathbf{Z} \\mathbf{x}_{22} \\\\\n",
    "\t\t\\mathbf{Z}x_{22} & \\mathbf{x}_{22}^2 \n",
    "\t\\end{bmatrix})\\right) \n",
    " &=\n",
    "  \\begin{bmatrix}\n",
    " \t(\\rho_0\\mathbf{x}_{22})^2 + 1-\\rho_0^2 & \\rho_0\\mathbf{x}_{22} \\mathbf{x}_{22} \\\\\n",
    " \t\\rho_0\\mathbf{x}_{22} \\mathbf{x}_{22} & \\mathbf{x}_{22}^2 \n",
    " \\end{bmatrix}\n",
    " =\n",
    " \\begin{bmatrix}\n",
    " \t(\\rho_0\\mathbf{x}_{22})^2 + 1-\\rho_0^2 & \\rho_0\\mathbf{x}_{22}^2 \\\\\n",
    " \t\\rho_0\\mathbf{x}_{22}^2 & \\mathbf{x}_{22}^2 \n",
    " \\end{bmatrix} \\\\\n",
    " \\mathbf{\\Sigma(\\rho)}^{-1} &= \\frac{1}{\\det(\\mathbf{\\Sigma(\\rho)})}\n",
    " \\begin{bmatrix}\n",
    " \t1 & -\\rho \\\\\n",
    " \t-\\rho & 1\n",
    " \\end{bmatrix} = \n",
    "\\frac{1}{1-\\rho^2}\n",
    "\\begin{bmatrix}\n",
    "\t1 & -\\rho \\\\\n",
    "\t-\\rho & 1\n",
    "\\end{bmatrix}\\\\\n",
    " \\text{trace}(\\mathbf{\\Sigma(\\rho)}^{-1} \n",
    " \\left( \n",
    " \\begin{bmatrix}\n",
    " \t(\\rho_0\\mathbf{x}_{22})^2 + 1-\\rho_0^2 & \\rho_0\\mathbf{x}_{22}^2 \\\\\n",
    " \t\\rho_0\\mathbf{x}_{22}^2 & \\mathbf{x}_{22}^2 \n",
    " \t\\end{bmatrix}\n",
    " \\right) \n",
    " &= \n",
    " \\text{trace}\\left( \n",
    " \\frac{1}{1-\\rho^2}\n",
    " \\begin{bmatrix}\n",
    " \t1 & -\\rho \\\\\n",
    " \t-\\rho & 1\n",
    " \\end{bmatrix}\n",
    " \\begin{bmatrix}\n",
    " \t(\\rho_0\\mathbf{x}_{22})^2 + 1-\\rho_0^2 & \\rho_0\\mathbf{x}_{22}^2 \\\\\n",
    " \t\\rho_0\\mathbf{x}_{22}^2 & \\mathbf{x}_{22}^2 \n",
    " \\end{bmatrix}\n",
    " \\right) \\\\\n",
    " &= \n",
    " \\frac{1}{1-\\rho^2}\n",
    " \\text{trace}\\left( \n",
    " \\begin{bmatrix}\n",
    " \t(\\rho_0\\mathbf{x}_{22})^2 + 1-\\rho_0^2 + \\rho \\rho_0 \\mathbf{x_{22}}^2 & \\rho_0\\mathbf{x}_{22}^2 - \\rho \\mathbf{x_{22}}^2 \\\\\n",
    " \t\\rho(\\rho_0\\mathbf{x}_{22}^2 +1 - \\rho_0)+\\rho_0\\mathbf{x_{22}}^2 & \\mathbf{x}_{22}^2 -\\rho \\rho_0\\mathbf{x_{22}}^2\n",
    " \\end{bmatrix}\n",
    " \\right) \\\\\n",
    " \\mathbb{E}\\left(\\mathbf{X_2}^\\top \\mathbf{\\Sigma(\\rho)}^{-1}\\mathbf{X_2} \\right)  &= \\frac{1}{1-\\rho^2} \\left( \\rho_0^2\\mathbf{x_{22}}^2+1- \\rho^2-2\\rho\\rho_0\\mathbf{x_{22}}^2+\\mathbf{x_{22}}^2\\right) \\\\\n",
    " \\implies \n",
    " \\mathbb{E}\\left( \\ell(\\rho)\\right)= \n",
    " -\\frac{3}{2}\\log(\\det\\left( 2\\pi \\mathbf{\\Sigma(\\rho)}\\right)) + \\sum_{i=\\lbrace 1,3 \\rbrace}\\frac{1}{2}\\mathbf{X_i}^\\top \\mathbf{\\Sigma(\\rho)}^{-1}\\mathbf{X_i}\n",
    " &+ \\frac{1}{2(1-\\rho^2)} \\left( \\rho_0^2\\mathbf{x_{22}}^2+1- \\rho^2-2\\rho\\rho_0\\mathbf{x_{22}}^2+\\mathbf{x_{22}}^2\\right) \\\\\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1cd36c403dde3a532a877a43ad92522",
     "grade": false,
     "grade_id": "cell-46bf29d7d4d92271",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 2: Extension of 'simple example' from the lecture\n",
    "Suppose that we have $N$ independent observations $x = ( x_1, \\dots, x_N )$ from a two-component mixture of univariate Gaussian distributions with unknown mixing co-efficients and unknown mean of the second component:\n",
    "$$ p(x_{n} \\mid \\theta,\\tau)=(1-\\tau)\\mathcal{N}(x_{n}|0,1)+\\tau\\mathcal{N}(x_{n} \\mid \\theta,1).$$\n",
    "\n",
    "**(a)** Write down the complete data log-likelihood and derive the EM-algorithm for learning the maximum likelihood estimates for $\\theta$ and $\\tau$. \n",
    "\n",
    "**(b)** Simulate some data from the model ($N = 100$ samples) with the true values of parameters $\\theta$ = 3 and $\\tau = 0.5$. Run your EM algorithm to see whether the learned parameters converge close to the true values (by e.g. just listing the estimates from a few iterations or plotting them). Use the code template below (after the answer cell) as a starting point. \n",
    "\n",
    "**HINT**: The E and M steps for simple example.pdf from the lecture material looks as follows\n",
    "```Python\n",
    "\t# E-step: compute the responsibilities r2 for component 2\n",
    "\tr1_unnorm = scipy.stats.norm.pdf(x, 0, 1)\n",
    "\tr2_unnorm = scipy.stats.norm.pdf(x, theta_0, 1)\n",
    "\tr2 = r2_unnorm / (r1_unnorm + r2_unnorm)\n",
    "\t\n",
    "\t# M-step: compute the parameter value that maximizes\n",
    "\t# the expectation of the complete-data log-likelihood.\n",
    "\ttheta[it] = sum(r2 * x) / sum(r2)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete log-likelihood can be calcualted as\n",
    "\n",
    "\\begin{align}\n",
    "\t\\ell(\\theta, \\tau) \n",
    "\t&= \\sum_{n=1}^{N} \\log p(x_n, z_n \\mid \\theta, \\tau) \\\\\n",
    "\t&= \\sum_{n=1}^{N} \\log p(x_n \\mid z_n, \\theta) + \\log p(z_n \\mid \\tau) \\\\\n",
    "\t&= \\sum_{n=1}^{N} \\log (1-\\tau) \\mathcal{N}(x_n \\mid 0,1)^{z_{n1}} + \\log (\\tau) \\mathcal{N}(x_n \\mid \\theta,1)^{z_{n2}} \\\\\n",
    "\t&= \\sum_{n=1}^{N} z_{n1}\\log \\mathcal{N}(x_n \\mid 0,1)  + z_{n2}\\log\\mathcal{N}(x_n \\mid \\theta,1) + \\log (\\tau) +\\log (1-\\tau)\n",
    "\\end{align}\n",
    "\n",
    "#### E-step\n",
    "\n",
    "\\begin{align}\n",
    "\tp(z_{n1} = 1 \\mid x_n, \\theta_0, \\tau_0) &\\propto p(z_{n1} = 1) p(x_n \\mid z_{n1}, \\theta_0, \\tau_0) \\\\\n",
    "\t&= (1-\\tau_0)\\mathcal{N}(x_n \\mid 0,1) \\\\\n",
    "\tp(z_{n1} = 1 \\mid x_n, \\theta_0, \\tau_0) &\\propto (\\tau_0)\\mathcal{N}(x_n \\mid \\theta_0,1)\n",
    "\\end{align}\n",
    "We also normalize the probabilities as \n",
    "\\begin{align}\n",
    "\t\\gamma(z_{n1}) = p(z_{n1} = 1 \\mid x_n, \\theta_0, \\tau_0) &= \\frac{(1-\\tau_0)\\mathcal{N}(x_n \\mid 0,1)}{(1-\\tau_0)\\mathcal{N}(x_n \\mid 0,1) + (\\tau_0)\\mathcal{N}(x_n \\mid \\theta_0,1)} \\\\\n",
    "\t\\gamma(z_{n2}) = p(z_{n2} = 1 \\mid x_n, \\theta_0, \\tau_0) &= \\frac{(\\tau_0)\\mathcal{N}(x_n \\mid \\theta_0,1)}{(1-\\tau_0)\\mathcal{N}(x_n \\mid 0,1) + (\\tau_0)\\mathcal{N}(x_n \\mid \\theta_0,1)}\n",
    "\\end{align}\n",
    "\n",
    "Since we have a bivariate distribution, we can note the responsibilites $\\gamma(z_{n2}) = 1- \\gamma(z_{n1})$\n",
    "\n",
    "\\begin{align}\n",
    "\tQ(\\theta, \\tau \\mid \\theta_0, \\tau_0) =& \\mathbb{E}_{z} \\left(\\sum_{n=1}^{N} z_{n1}\\log \\mathcal{N}(x_n \\mid 0,1)  + z_{n2}\\log\\mathcal{N}(x_n \\mid \\theta,1) + z_{n2}\\log (\\tau) + z_{n1}\\log(1-\\tau)\\right) \\\\\n",
    "\t=& \\sum_{n=1}^{N} \\mathbb{E}(z_{n1})\\log \\mathcal{N}(x_n \\mid 0,1)  + \\mathbb{E}(z_{n2})\\log\\mathcal{N}(x_n \\mid \\theta,1) + \\mathbb{E}(z_{n2})\\log (\\tau) + \\mathbb{E}(z_{n1})\\log (1-\\tau)\\\\\n",
    "\t=& \\sum_{n=1}^{N} \\gamma(z_{n1})\\log \\mathcal{N}(x_n \\mid 0,1)  + (1-\\gamma(z_{n1}))\\log\\mathcal{N}(x_n \\mid \\theta,1) + (1-\\gamma(z_{n1}))\\log (\\tau) + \\gamma(z_{n1})\\log (1-\\tau)\n",
    "\\end{align}\n",
    "\n",
    "#### M-step\n",
    "\n",
    "We note that $\\frac{d}{d\\mu}\\mathcal{N}(x_n|\\mu,1) = \\mathcal{N}(x_n|\\mu,1)(x_n - \\mu)$\n",
    "\n",
    "For $\\theta$:\n",
    "\n",
    "\\begin{align}\n",
    "\t\\frac{d}{d\\theta}Q(\\theta, \\tau \\mid \\theta_0) =& \n",
    "\t\\sum_{n=1}^{N} \\frac{d}{d\\theta}(1-\\gamma(z_{n1}))\\log\\mathcal{N}(x_n \\mid \\theta,1) \\\\\n",
    "\t=& \\sum_{n=1}^{N} (1-\\gamma(z_{n1}))(x_n - \\theta)\\\\\n",
    "\t\\frac{d}{d\\theta}Q(\\theta, \\tau \\mid \\theta_0) = 0 \\iff 0 =& \\sum_{n=1}^{N} (1-\\gamma(z_{n1}))(x_n - \\theta) \\\\\n",
    "\t=& \\sum_{n=1}^{N}(1-\\gamma(z_{n1}))x_n - \\sum_{n=1}^{N}(1-\\gamma(z_{n1}))\\theta\\\\\n",
    "\t\\sum_{n=1}^{N}(1-\\gamma(z_{n1}))\\theta =& \\sum_{n=1}^{N}(1-\\gamma(z_{n1}))x_n \\\\\n",
    "\t\\theta =& \\frac{1}{N_2}\\sum_{n=1}^{N}(1-\\gamma(z_{n1}))x_n, \\qquad | N_2 = \\sum_{n=1}^{N}(1-\\gamma(z_{n1}))\n",
    "\\end{align}\n",
    "\n",
    "For $\\tau$:\n",
    "\n",
    "\\begin{align}\n",
    "\t\\frac{d}{d\\tau}Q(\\theta, \\tau \\mid \\theta_0) =& \\sum_{n=1}^{N} \\frac{d}{d\\tau} (1-\\gamma(z_{n1}))\\log (\\tau) + \\gamma(z_{n1})\\log (1-\\tau) \\\\\n",
    "\t&= \\sum_{n=1}^{N} \\frac{1-\\gamma(z_{n1})}{\\tau} +  \\frac{\\gamma(z_{n1})}{1-\\tau} = \\frac{N_2}{\\tau} + \\frac{N_1}{1-\\tau}\\\\\n",
    "\t&\\iff \\\\\n",
    "\t\\tau &= \\frac{N_2}{N_1+N_2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b479fd715d72b0417e4f3ac4c0f8e914",
     "grade": false,
     "grade_id": "cell-1abac854e88e7dc1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta       tau\n",
      "1.0000000  0.1000000\n",
      "3.2393002  38.5929521\n",
      "3.2223708  61.1837756\n",
      "3.2303810  60.8841014\n",
      "3.2245561  61.1086403\n",
      "3.2287852  60.9453138\n",
      "3.2257118  61.0638476\n",
      "3.2279438  60.9776821\n",
      "3.2263221  61.0402436\n",
      "3.2274999  60.9947810\n",
      "3.2266442  61.0277973\n",
      "3.2272658  61.0038090\n",
      "3.2268142  61.0212322\n",
      "3.2271422  61.0085744\n",
      "3.2269040  61.0177686\n",
      "3.2270770  61.0110894\n",
      "3.2269513  61.0159411\n",
      "3.2270426  61.0124166\n",
      "3.2269763  61.0149768\n",
      "3.2270245  61.0131170\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# template for Problem 2(b)\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "### Simulate data:\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "theta_true = 3\n",
    "tau_true = 0.5\n",
    "n_samples = 100\n",
    "\n",
    "x = np.zeros(n_samples)\n",
    "for i in range(n_samples):\n",
    "    # Sample from N(0,1) or N(theta_true,1)\n",
    "    if np.random.rand() < 1 - tau_true:\n",
    "        x[i] = np.random.normal(0, 1)\n",
    "    else:\n",
    "        x[i] = np.random.normal(theta_true, 1)\n",
    "\n",
    "\n",
    "### The EM algorithm:\n",
    "\n",
    "n_iter = 20\n",
    "theta = np.zeros(n_iter)\n",
    "tau = np.zeros(n_iter)\n",
    "\n",
    "# Initial guesses for theta and tau\n",
    "theta[0] = 1\n",
    "tau[0] = 0.1\n",
    "\n",
    "for it in range(1, n_iter):\n",
    "    # The current estimates for theta and tau,\n",
    "    # computed in the previous iteration\n",
    "    theta_0 = theta[it-1]\n",
    "    tau_0 = tau[it-1]\n",
    "\n",
    "    # E-step: compute the responsibilities r1 and r2\n",
    "    r1_unnorm = (1-tau_0)*scipy.stats.norm.pdf(x, 0, 1)\n",
    "    r2_unnorm = (tau_0)*scipy.stats.norm.pdf(x, theta_0, 1)\n",
    "    r1 = r1_unnorm / (r1_unnorm + r2_unnorm)\n",
    "    r2 = r2_unnorm / (r1_unnorm + r2_unnorm)\n",
    "\n",
    "    # M-step: compute the parameter values that maximize\n",
    "    # the expectation of the complete-data log-likelihood.\n",
    "    theta[it] = sum(r2 * x) / sum(r2)\n",
    "    tau[it] = sum(r2) / sum(r1) + sum(r2)\n",
    "    \n",
    "\n",
    "# Print and plot the values of theta and tau in each iteration\n",
    "print(\"theta       tau\")\n",
    "for theta_i, tau_i in zip(theta, tau):\n",
    "    print(\"{0:.7f}  {1:.7f}\".format(theta_i, tau_i))\n",
    "\n",
    "plt.plot(range(n_iter), theta, label = 'theta')\n",
    "plt.plot(range(n_iter), tau, label = 'tau')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82083e7d021d449db017e6010251e7ad",
     "grade": false,
     "grade_id": "cell-482274cb8fbd6887",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Problem 3: PyTorch\n",
    "Go through the PyTorch tutorials in the three links and answer the questions given below\n",
    "\n",
    "1) What is PyTorch: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    "\n",
    "2) Autograd: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py\n",
    "\n",
    "3) Linear regression with PyTorch: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/linear_regression/main.py\n",
    "\n",
    "__(a)__ What are PyTorch Tensors and how do you run a CPU tensor on GPU? \n",
    "\n",
    "\n",
    "__(b)__ What is Automatic differentiation and autograd? \n",
    "\n",
    "\n",
    "__(c)__ PyTorch constructs the computation graph dynamically as the operations are defined. In the 'linear regression with PyTorch' tutorial which line numbers indicates the completion of the computation graph, computation of the gradients and update of the weights, respectively? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to Problem 3 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(a)__: PyTorch tensors are a special data struct that can be run on GPUs or specialized hardware. In order to run Tensors on the GPU you use the command tensor.to(\"cuda\")\n",
    "\n",
    "__(b)__: Autograd is a automatic differentiation engine for computing vector jacobian products. Automatic differentiation uses the DAG of the model to compute the gradient of the model using the chain rule. The result is then saved in a DAG for later use. \n",
    "\n",
    "__(c)__: \n",
    "* \"the completion of the computation graph\": 27\n",
    "* \"computation of the gradients\": 40-41\n",
    "* \"update of the weights\": 42"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
